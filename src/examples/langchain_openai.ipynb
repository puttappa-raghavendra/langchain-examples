{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# openai key is loaded in .env file. load the environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI model\n",
    "Two model - OpenAI and ChatOpenAI\n",
    "With OpenAI, the input and output are strings, while with ChatOpenAI, the input is a sequence of messages and the output is a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "# crate llm using openai \n",
    "llm = OpenAI(openai_api_key=getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "I am a digital assistant and do not have emotions. I am always functioning at my best and ready to assist you with any tasks or questions you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = llm.invoke(\"how are you?\")\n",
    "\n",
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "AI Message:=  content=\"I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you with whatever you need. How can I assist you today?\" response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 11, 'total_tokens': 46}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-246125b8-2a45-4471-8e88-527365956b45-0'\n",
      "Content:=  I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you with whatever you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# chatopenai model\n",
    "chat_llm = ChatOpenAI(openai_api_key=getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chat_res = chat_llm.invoke(\"how are you?\")\n",
    "\n",
    "print(type(chat_res))\n",
    "print(\"AI Message:= \", chat_res)\n",
    "print(\"Content:= \", chat_res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response:=  Hello raghav, to book a flight, you can follow these steps:\n",
      "\n",
      "1. Visit a travel website or airline website.\n",
      "2. Enter your departure city, destination, dates of travel, and number of passengers.\n",
      "3. Browse through available flight options and choose the one that suits your preferences and budget.\n",
      "4. Enter passenger details and payment information.\n",
      "5. Review the booking details and confirm your flight reservation.\n",
      "6. Once the booking is confirmed, you will receive a confirmation email with your flight details.\n",
      "\n",
      "Let me know if you need any more assistance with booking your flight.\n"
     ]
    }
   ],
   "source": [
    "# chatopen ai llm accept multiple messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# chat history\n",
    "# chain of messages and chat history\n",
    "messages = [\n",
    "    HumanMessage(\"Hello, my name is raghav. when you answer my questions, start addressing my name followed by my name\"),\n",
    "    SystemMessage(\"Hello raghav, how can I help you?\"),\n",
    "    HumanMessage(\"do any know steps to book flight?\"),\n",
    "]\n",
    "\n",
    "res = chat_llm.invoke(messages)\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parser - parse the AIMessage using different parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'str'>\n",
      "Response:=  Hello raghav, to book a flight, you can follow these steps:\n",
      "\n",
      "1. Visit a flight booking website or app.\n",
      "2. Enter your departure city, destination city, travel dates, and number of passengers.\n",
      "3. Browse through the available flight options and choose the one that suits your preferences.\n",
      "4. Enter passenger details and contact information.\n",
      "5. Select any additional services or preferences (such as seat selection or meal preferences).\n",
      "6. Proceed to the payment page and enter your payment details.\n",
      "7. Confirm your booking and check your email for the e-ticket.\n",
      "\n",
      "Is there anything else you would like to know?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LCEL\n",
    "llm_chat_str_parser = chat_llm | StrOutputParser()\n",
    "\n",
    "res = llm_chat_str_parser.invoke(messages)\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response:=  {\n",
      "    \"cpu\": \"20%\",\n",
      "    \"memory\": \"30%\",\n",
      "    \"storage\": \"40%\",\n",
      "    \"network\": \"50%\"\n",
      "}\n",
      "Content Type:=  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat prompt template\n",
    "template = '''You are system administractor who can understand the resource utilization metrics of a system like cpu utilization, memory utilization, storage utilization and network utilization.\n",
    "Translate the metrics details in below json format: \n",
    "All the metrics values must be in exact percentage.\n",
    "{{\n",
    "    \"cpu\": \"cpu utilization in percentage\",\n",
    "    \"memory\": \"memory utilization in percentage\",\n",
    "    \"storage\": \"storage utilization in percentage\",\n",
    "    \"network\": \"network utilization in percentage\"\n",
    "}}\n",
    "system utilization metrics in unstructured format:\n",
    "{resource_utilization}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template=template, input_args=[\"resource_utilization\"])\n",
    "\n",
    "llm_prompt = prompt | chat_llm\n",
    "\n",
    "res = llm_prompt.invoke({\"resource_utilization\": \"cpu utilization is 20%, memory utilization is 30%, storage utilization is 40%, network utilization is 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res.content)\n",
    "print(\"Content Type:= \", type(res.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'dict'>\n",
      "Response:=  {'cpu': '20%', 'memory': '30%', 'storage': '40%', 'network': '50%'}\n",
      "CPU:=  20%\n"
     ]
    }
   ],
   "source": [
    "# parse the output using jsonparser \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm_prompt_json_parser = llm_prompt | JsonOutputParser()\n",
    "\n",
    "res = llm_prompt_json_parser.invoke({\"resource_utilization\": \"cpu utilization is 20%, memory utilization is 30%, storage utilization is 40%, network utilization is 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)\n",
    "print(\"CPU:= \", res['cpu'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse output to Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"cpu\": {\"title\": \"Cpu\", \"description\": \"cpu utilization in percentage\", \"type\": \"number\"}, \"memory\": {\"title\": \"Memory\", \"description\": \"memory utilization in percentage\", \"type\": \"number\"}, \"storage\": {\"title\": \"Storage\", \"description\": \"storage utilization in percentage\", \"type\": \"number\"}, \"network\": {\"title\": \"Network\", \"description\": \"network utilization in percentage\", \"type\": \"number\"}}, \"required\": [\"cpu\", \"memory\", \"storage\", \"network\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class ResourceUtilization(BaseModel):\n",
    "    cpu: float = Field(..., description=\"cpu utilization in percentage\")\n",
    "    memory: float = Field(..., description=\"memory utilization in percentage\")\n",
    "    storage: float = Field(..., description=\"storage utilization in percentage\")\n",
    "    network: float = Field(..., description=\"network utilization in percentage\")\n",
    "    \n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ResourceUtilization)\n",
    "\n",
    "print( pydantic_parser.get_format_instructions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM with Pydantic Parser integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class '__main__.ResourceUtilization'>\n",
      "Response:=  cpu=20.0 memory=30.0 storage=40.0 network=50.0\n",
      "CPU:=  20.0\n"
     ]
    }
   ],
   "source": [
    "# chat prompt template\n",
    "template = '''You are system administractor who can understand the resource utilization metrics of a system like cpu utilization, memory utilization, storage utilization and network utilization.\n",
    "{format_instructions}\n",
    "System utilization metrics in unstructured format:\n",
    "{resource_utilization}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template=template, \n",
    "                                          input_args=[\n",
    "                                              \"resource_utilization\",\n",
    "                                              \"format_instructions\"\n",
    "                                            ]\n",
    "                                          )\n",
    "\n",
    "llm_prompt_json_parser = prompt | chat_llm | pydantic_parser\n",
    "\n",
    "res = llm_prompt_json_parser.invoke({\n",
    "    \"format_instructions\": pydantic_parser.get_format_instructions(),\n",
    "    \"resource_utilization\": \"cpu utilization is 20%, memory = 30%, storage utilization is 40%, network = 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)\n",
    "print(\"CPU:= \", res.cpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
