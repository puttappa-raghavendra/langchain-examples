{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# openai key is loaded in .env file. load the environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI model\n",
    "Two model - OpenAI and ChatOpenAI.\n",
    "OpenAI model, the input and output are strings, while with ChatOpenAI model, the input is a sequence of messages and the output is a AIMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "# crate llm using openai \n",
    "llm = OpenAI(openai_api_key=getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "I am a digital assistant and do not have emotions. I am always functioning at my best and ready to assist you with any tasks or questions you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = llm.invoke(\"how are you?\")\n",
    "\n",
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "AI Message:=  content=\"I'm just a computer program, so I don't have feelings like humans do. But I'm here to help you with anything you need. How can I assist you today?\" response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 11, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-dca72145-464e-4055-8c96-b7628b683424-0'\n",
      "Content:=  I'm just a computer program, so I don't have feelings like humans do. But I'm here to help you with anything you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# chatopenai model\n",
    "chat_llm = ChatOpenAI(openai_api_key=getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chat_res = chat_llm.invoke(\"how are you?\")\n",
    "\n",
    "print(type(chat_res))\n",
    "print(\"AI Message:= \", chat_res)\n",
    "print(\"Content:= \", chat_res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response:=  Hello raghav, to book a flight, you can follow these steps:\n",
      "\n",
      "1. Visit a flight booking website or app.\n",
      "2. Enter your departure city, destination city, dates of travel, and number of passengers.\n",
      "3. Browse through the available flight options and select the one that suits your preferences.\n",
      "4. Enter passenger details and contact information.\n",
      "5. Choose any additional services or add-ons you may need, such as seat selection or extra baggage.\n",
      "6. Review your booking details, including the total cost.\n",
      "7. Proceed to payment and enter your payment information.\n",
      "8. Once the payment is confirmed, you will receive a booking confirmation with your flight details.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "# chatopen ai llm accept multiple messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# chat history\n",
    "# chain of messages and chat history\n",
    "messages = [\n",
    "    HumanMessage(\"Hello, my name is raghav. when you answer my questions, start addressing by my name\"),\n",
    "    SystemMessage(\"Hello raghav, how can I help you?\"),\n",
    "    HumanMessage(\"do any know steps to book flight?\"),\n",
    "]\n",
    "\n",
    "res = chat_llm.invoke(messages)\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "print( prompt_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parser - parse the AIMessage using different parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'str'>\n",
      "Response:=  Hello raghav, to book a flight, you can follow these steps:\n",
      "\n",
      "1. Visit a flight booking website or app.\n",
      "2. Enter your departure city, destination city, travel dates, and the number of passengers.\n",
      "3. Browse available flights and select the one that best fits your schedule and budget.\n",
      "4. Enter passenger details and payment information to complete the booking.\n",
      "5. Once the booking is confirmed, you will receive a confirmation email with your flight details.\n",
      "\n",
      "If you need any further assistance, feel free to ask, raghav.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LCEL\n",
    "llm_chat_str_parser = chat_llm | StrOutputParser() # LCEL\n",
    "\n",
    "res = llm_chat_str_parser.invoke(messages)\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response:=  {\n",
      "    \"cpu\": 20,\n",
      "    \"memory\": \"30%\",\n",
      "    \"storage\": 40,\n",
      "    \"network\": 50\n",
      "}\n",
      "Content Type:=  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat prompt template\n",
    "template = '''You are system admin who can understand the resource utilization metrics of a server or system like cpu utilization, memory utilization, storage utilization and network utilization in different format.\n",
    "\n",
    "Translate the metrics details in below json format: \n",
    "All the metrics values must be in exact percentage.\n",
    "{{\n",
    "    \"cpu\": \"cpu utilization in percentage and of type integer\",\n",
    "    \"memory\": \"memory utilization in percentage is in string with percentage\",\n",
    "    \"storage\": \"storage utilization type integer\",\n",
    "    \"network\": \"network utilization in percentage\"\n",
    "}}\n",
    "\n",
    "system utilization metrics in unstructured format:\n",
    "\n",
    "{resource_utilization}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template=template, input_args=[\"resource_utilization\"])\n",
    "\n",
    "llm_prompt = prompt | chat_llm\n",
    "\n",
    "res = llm_prompt.invoke({\"resource_utilization\": \"cpu consumption is 20%, memory is 30%, storage = 40%, network utilization is 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res.content)\n",
    "print(\"Content Type:= \", type(res.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'dict'>\n",
      "Response:=  {'cpu': 20, 'memory': '30%', 'storage': 40, 'network': 50}\n",
      "CPU:=  20\n"
     ]
    }
   ],
   "source": [
    "# parse the output using jsonparser \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm_prompt_json_parser = llm_prompt | JsonOutputParser()\n",
    "\n",
    "res = llm_prompt_json_parser.invoke({\"resource_utilization\": \"cpu utilization is 20%, memory utilization is 30%, storage utilization is 40%, network utilization is 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)\n",
    "print(\"CPU:= \", res['cpu'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse output to Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"cpu\": {\"title\": \"Cpu\", \"description\": \"cpu utilization in percentage\", \"type\": \"string\"}, \"memory\": {\"title\": \"Memory\", \"description\": \"memory utilization in percentage\", \"type\": \"string\"}, \"storage\": {\"title\": \"Storage\", \"description\": \"storage utilization in percentage\", \"type\": \"string\"}, \"network\": {\"title\": \"Network\", \"description\": \"network utilization in percentage\", \"type\": \"string\"}}, \"required\": [\"cpu\", \"memory\", \"storage\", \"network\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class ResourceUtilization(BaseModel):\n",
    "    cpu: str = Field(..., description=\"cpu utilization in percentage\")\n",
    "    memory: str = Field(..., description=\"memory utilization in percentage\")\n",
    "    storage: str = Field(..., description=\"storage utilization in percentage\")\n",
    "    network: str = Field(..., description=\"network utilization in percentage\")\n",
    "    \n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ResourceUtilization)\n",
    "\n",
    "print( pydantic_parser.get_format_instructions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM with Pydantic Parser integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class '__main__.ResourceUtilization'>\n",
      "Response:=  cpu='20%' memory='30%' storage='40%' network='50%'\n",
      "CPU:=  20%\n"
     ]
    }
   ],
   "source": [
    "# chat prompt template\n",
    "template = '''You are system admin who can understand the resource unstructured utilization metrics of a server or system like cpu utilization, memory utilization, storage utilization and network utilization.\n",
    "{format_instructions}\n",
    "System utilization metrics in unstructured format:\n",
    "{resource_utilization}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template=template, \n",
    "                                          input_args=[\"resource_utilization\"], \n",
    "                                          partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()})\n",
    "\n",
    "llm_prompt_json_parser = prompt | chat_llm | pydantic_parser\n",
    "\n",
    "res = llm_prompt_json_parser.invoke({\"resource_utilization\": \"cpu utilization is 20%, memory = 30%, storage utilization is 40%, network = 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)\n",
    "print(\"CPU:= \", res.cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
