{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openai key is loaded in .env file. load the environment variables\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "# loadenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI model\n",
    "Two model - OpenAI and ChatOpenAI\n",
    "With OpenAI, the input and output are strings, while with ChatOpenAI, the input is a sequence of messages and the output is a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "# crate llm using openai \n",
    "llm = OpenAI(openai_api_key=getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "I am an AI assistant and do not have emotions, so I am always doing well. Thank you for asking. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "res = llm.invoke(\"how are you?\")\n",
    "\n",
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "AI Message:=  content=\"I'm just a computer program, so I don't have feelings or emotions, but I'm here and ready to assist you with any questions or tasks you have. How can I help you today?\" response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 11, 'total_tokens': 51}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None} id='run-20525c03-c88f-465e-b710-cd418a7c4fce-0'\n",
      "Content:=  I'm just a computer program, so I don't have feelings or emotions, but I'm here and ready to assist you with any questions or tasks you have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# chatopenai model\n",
    "chat_llm = ChatOpenAI(openai_api_key=getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chat_res = chat_llm.invoke(\"how are you?\")\n",
    "\n",
    "print(type(chat_res))\n",
    "print(\"AI Message:= \", chat_res)\n",
    "print(\"Content:= \", chat_res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response:=  Hello raghav, to book a flight, you can follow these steps:\n",
      "\n",
      "1. Visit a flight booking website or app.\n",
      "2. Enter your departure city, destination, travel dates, and number of passengers.\n",
      "3. Browse through the available flights and select the one that suits your preferences.\n",
      "4. Enter passenger details and payment information.\n",
      "5. Review your booking details and confirm your flight reservation.\n",
      "6. Receive a confirmation email with your flight details.\n",
      "\n",
      "Is there anything else you would like to know about booking flights?\n"
     ]
    }
   ],
   "source": [
    "# chatopen ai llm accept multiple messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# chat history\n",
    "# chain of messages and chat history\n",
    "messages = [\n",
    "    HumanMessage(\"Hello, my name is raghav. when you answer my questions, start addressing my name followed by my name\"),\n",
    "    SystemMessage(\"Hello raghav, how can I help you?\"),\n",
    "    HumanMessage(\"do any know steps to book flight?\"),\n",
    "]\n",
    "\n",
    "res = chat_llm.invoke(messages)\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parser - parse the AIMessage using different parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'str'>\n",
      "Response:=  Hello raghav, to book a flight, you can follow these steps:\n",
      "1. Visit a flight booking website or app.\n",
      "2. Enter your departure city, destination, dates of travel, and number of passengers.\n",
      "3. Browse through available flight options and select the one that suits your preferences and budget.\n",
      "4. Enter passenger details and payment information.\n",
      "5. Review your booking details and confirm your flight reservation.\n",
      "6. Receive a confirmation email with your flight itinerary.\n",
      "\n",
      "Is there anything else you would like to know about booking a flight?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LCEL\n",
    "llm_chat_str_parser = chat_llm | StrOutputParser()\n",
    "\n",
    "res = llm_chat_str_parser.invoke(messages)\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response:=  {\n",
      "    \"cpu\": \"20%\",\n",
      "    \"memory\": \"30%\",\n",
      "    \"storage\": \"40%\",\n",
      "    \"network\": \"50%\"\n",
      "}\n",
      "Content Type:=  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat prompt template\n",
    "template = '''You are system administractor who can understand the resource utilization metrics of a system like cpu utilization, memory utilization, storage utilization and network utilization.\n",
    "Translate the metrics details in below json format: \n",
    "All the metrics values must be in exact percentage.\n",
    "{{\n",
    "    \"cpu\": \"cpu utilization in percentage\",\n",
    "    \"memory\": \"memory utilization in percentage\",\n",
    "    \"storage\": \"storage utilization in percentage\",\n",
    "    \"network\": \"network utilization in percentage\"\n",
    "}}\n",
    "system utilization metrics in unstructured format:\n",
    "{resource_utilization}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template=template, input_args=[\"resource_utilization\"])\n",
    "\n",
    "llm_prompt = prompt | chat_llm\n",
    "\n",
    "res = llm_prompt.invoke({\"resource_utilization\": \"cpu utilization is 20%, memory utilization is 30%, storage utilization is 40%, network utilization is 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res.content)\n",
    "print(\"Content Type:= \", type(res.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class 'dict'>\n",
      "Response:=  {'cpu': '20%', 'memory': '30%', 'storage': '40%', 'network': '50%'}\n",
      "CPU:=  20%\n"
     ]
    }
   ],
   "source": [
    "# parse the output using jsonparser \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm_prompt_json_parser = llm_prompt | JsonOutputParser()\n",
    "\n",
    "res = llm_prompt_json_parser.invoke({\"resource_utilization\": \"cpu utilization is 20%, memory utilization is 30%, storage utilization is 40%, network utilization is 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)\n",
    "print(\"CPU:= \", res['cpu'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse output to Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"cpu\": {\"title\": \"Cpu\", \"description\": \"cpu utilization in percentage\", \"type\": \"number\"}, \"memory\": {\"title\": \"Memory\", \"description\": \"memory utilization in percentage\", \"type\": \"number\"}, \"storage\": {\"title\": \"Storage\", \"description\": \"storage utilization in percentage\", \"type\": \"number\"}, \"network\": {\"title\": \"Network\", \"description\": \"network utilization in percentage\", \"type\": \"number\"}}, \"required\": [\"cpu\", \"memory\", \"storage\", \"network\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class ResourceUtilization(BaseModel):\n",
    "    cpu: float = Field(..., description=\"cpu utilization in percentage\")\n",
    "    memory: float = Field(..., description=\"memory utilization in percentage\")\n",
    "    storage: float = Field(..., description=\"storage utilization in percentage\")\n",
    "    network: float = Field(..., description=\"network utilization in percentage\")\n",
    "    \n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ResourceUtilization)\n",
    "\n",
    "print( pydantic_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM with Pydantic Parser integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message type:=  <class '__main__.ResourceUtilization'>\n",
      "Response:=  cpu=20.0 memory=30.0 storage=40.0 network=50.0\n",
      "CPU:=  20.0\n"
     ]
    }
   ],
   "source": [
    "# chat prompt template\n",
    "template = '''You are system administractor who can understand the resource utilization metrics of a system like cpu utilization, memory utilization, storage utilization and network utilization.\n",
    "{format_instructions}\n",
    "System utilization metrics in unstructured format:\n",
    "{resource_utilization}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template=template, \n",
    "                                          input_args=[\n",
    "                                              \"resource_utilization\",\n",
    "                                              \"format_instructions\"\n",
    "                                            ]\n",
    "                                          )\n",
    "\n",
    "llm_prompt_json_parser = prompt | chat_llm | pydantic_parser\n",
    "\n",
    "res = llm_prompt_json_parser.invoke({\n",
    "    \"format_instructions\": pydantic_parser.get_format_instructions(),\n",
    "    \"resource_utilization\": \"cpu utilization is 20%, memory = 30%, storage utilization is 40%, network = 50%\"})\n",
    "\n",
    "print(\"ChatOpenAI message type:= \", type(res))\n",
    "print(\"Response:= \", res)\n",
    "print(\"CPU:= \", res.cpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
